
agent.sources = s1    
agent.channels = c1  
agent.sinks = k1 

agent.sources.s1.type=exec
agent.sources.s1.command=hadoop fs -cat /random/logs.dat | head -200000
agent.sources.s1.channels=c1

agent.channels.c1.type=memory
agent.channels.c1.capacity=10000
agent.channels.c1.transactionCapacity=100


#设置Kafka接收器  
agent.sinks.k1.type= org.apache.flume.sink.kafka.KafkaSink  
#设置Kafka的broker地址和端口号  
agent.sinks.k1.brokerList=116.56.136.57:9092
#设置Kafka的Topic  
agent.sinks.k1.topic=data  
#设置序列化方式  
agent.sinks.k1.serializer.class=kafka.serializer.StringEncoder  
  
agent.sinks.k1.channel=c1
